{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DjZ-LUnMoA86"
   },
   "source": [
    "# Importing modules and data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing all modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warning: if you have an error message: \"ImportError: No module named [module name]\", please go to your command line and launch command: pip install [module name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "akVpN9h1Vun0"
   },
   "outputs": [],
   "source": [
    "# module to get current directory folder\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# module to import global parameters from JSON file\n",
    "import json\n",
    "\n",
    "# module for progress bars\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# date and time manipulation\n",
    "import datetime as dt\n",
    "import pytz\n",
    "tz = pytz.timezone(\"Etc/GMT\")\n",
    "\n",
    "# data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import compress\n",
    "\n",
    "# technical (financial) indicators \n",
    "import ta  \n",
    "\n",
    "# modules for data preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# pytorch machine learning models\n",
    "import torch\n",
    "import torch.autograd as autograd\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# plotting module\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing sentiment of tweets dataset and price dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "oLQnj4xyVuoU"
   },
   "outputs": [],
   "source": [
    "# getting bitcoin's price data\n",
    "with open(os.path.join(sys.path[0],\"data/binance_BTCUSDT_1m.csv\")) as fp:\n",
    "    raw_bitcoin_data = pd.read_csv(fp)\n",
    "    \n",
    "# converting string of date &time to python datetime\n",
    "raw_bitcoin_data['time']=raw_bitcoin_data['time'].apply(lambda utc: tz.localize(dt.datetime.utcfromtimestamp(utc)))\n",
    "\n",
    "# getting sentiment analysis of every tweets in tweets csv\n",
    "with open(os.path.join(sys.path[0],\"data/sentiments.csv\")) as fp:\n",
    "    raw_sentiment_data = pd.read_csv(fp)\n",
    "    \n",
    "# converting string of date & time to python datetime\n",
    "raw_sentiment_data['time'] = pd.to_datetime(raw_sentiment_data['time'],errors='coerce')\n",
    "\n",
    "# removing errors (file isn't always well formated)\n",
    "raw_sentiment_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Q_9eC5PoK9B"
   },
   "source": [
    "## Function to initialize dataframe with wanted features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "McQ7c59UVuoV"
   },
   "outputs": [],
   "source": [
    "# main function that preprocesses the data to single dataframe\n",
    "def initialize(raw_sentiment, raw_prices, time_int,rsi_window):\n",
    "    \n",
    "    # grouping values by time invertval and computing: \n",
    "        # opening value (getting opening value of first element)\n",
    "        # highest value (getting max in interval)\n",
    "        # lowest value (getting min in interval)\n",
    "        # close value (getting closing value of last element)\n",
    "        # volume (summing all volume exchanged in interval)\n",
    "    bitcoin_data = raw_prices.groupby(pd.Grouper(key='time',freq=f\"{time_int}\")).agg({\n",
    "        \"open\": \"first\",\n",
    "        \"high\":\"max\",\n",
    "        \"low\":\"min\", \n",
    "        \"close\": \"last\", \n",
    "        \"volume\": \"sum\"\n",
    "    })\n",
    "    \n",
    "    # calculating rsi index\n",
    "    bitcoin_data['rsi'] = ta.momentum.RSIIndicator(close=bitcoin_data.close, window=rsi_window).rsi()\n",
    "    \n",
    "    # calculating variation in time interval\n",
    "    bitcoin_var = (bitcoin_data['open']-bitcoin_data['close'])/bitcoin_data['open']\n",
    "    \n",
    "    # applying classification: 1: variation > 0, 0: variation =< 0\n",
    "    bitcoin_class = bitcoin_var.apply(lambda x: 1 if x>0 else 0)\n",
    "    \n",
    "    # generating batch of the sentiment of tweets per time interval\n",
    "    tweets_per_int = raw_sentiment.groupby(pd.Grouper(key='time',freq=f\"{time_int}\"))\n",
    "    \n",
    "    # getting mean polarity scores for each time interval\n",
    "    polarity_scores = tweets_per_int.mean()\n",
    "    \n",
    "    # creating final dataset\n",
    "    df = pd.DataFrame(polarity_scores)\n",
    "    \n",
    "    # computing number of tweets per time interval\n",
    "    df['tweets'] = tweets_per_int.neg.count()\n",
    "    \n",
    "    # adding price and indicators data to final dataframe\n",
    "    df[bitcoin_data.columns] = bitcoin_data\n",
    "    \n",
    "    # adding label (price going up or down)\n",
    "    df['label']= bitcoin_class\n",
    "    \n",
    "    # dropping unwanted columns\n",
    "    df.drop([\"Unnamed: 0\",'neu','compound','open','high','low'],inplace=True, axis=1)\n",
    "    \n",
    "    # reseting index from date & time to numbers\n",
    "    df.reset_index(inplace=True,drop=True)\n",
    "    \n",
    "    # converting missing data (in format: NaN): \n",
    "        # negativity and positivity to 0\n",
    "        # closing, volume and rsi to last values\n",
    "    df.fillna({'neg':0,'pos':0},inplace=True)\n",
    "    df.close.ffill(inplace=True)\n",
    "    df.volume.ffill(inplace=True)\n",
    "    df.rsi.ffill(inplace=True)\n",
    "    ####### TROUVER MOYEN POUR NE PAS AVOIR DE SAUT DANS LE TEMPS: GROUPER PREND TOUTES LES VALUES\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BCsQLGgSomgy"
   },
   "source": [
    "## Getting data ready for LSTM Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "eLDX5k3O83ja"
   },
   "outputs": [],
   "source": [
    "# creating class dataset (needed for dataloader method)\n",
    "class Dataset(Dataset):\n",
    "    \n",
    "    # init method\n",
    "    def __init__(self,X,y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.len = X.shape[0]\n",
    "        \n",
    "    # get item method to get specific item of dataset\n",
    "    def __getitem__(self,idx):\n",
    "        return self.X[idx],self.y[idx]\n",
    "    \n",
    "    # len method to get length of dataset\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to create sequences (a feature set will have sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "PUW2BylAsXMd"
   },
   "outputs": [],
   "source": [
    "# function to create sequences from dataframe (LSTM treats sequences as input)\n",
    "def create_sequences(input_data: pd.DataFrame, target_column, sequence_length):\n",
    "    \n",
    "    # getting size of data\n",
    "    data_size = len(input_data)\n",
    "    \n",
    "    sequences =[]\n",
    "    \n",
    "    # getting features and label for every element:\n",
    "        # we get all features (polarity scores + price + indicators) of last x values and label for next line \n",
    "        # => we do not predict with data we will not have \n",
    "    for i in range(data_size-sequence_length):\n",
    "        \n",
    "        X_i = torch.Tensor(input_data.drop(target_column,axis=1).iloc[i:i+sequence_length].to_numpy())\n",
    "        \n",
    "        y_i = torch.Tensor([input_data.iloc[i+sequence_length][target_column]])\n",
    "        \n",
    "        sequences.append((X_i,y_i))\n",
    "        \n",
    "    # unpacking features and labels to different vectors\n",
    "    X = [sequence[0] for sequence in sequences]\n",
    "    y = [sequence[1] for sequence in sequences]\n",
    "    \n",
    "    return torch.stack(X),torch.stack(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to create dataloader object (that creates batches of data) from initial dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "C9_gctv5oZfG"
   },
   "outputs": [],
   "source": [
    "# function that converts initial dataframe to format it in an acceptable way to machine learning module  \n",
    "def df2loader(df,val_proportion,test_proportion, sequence_size, batch_size):\n",
    "\n",
    "    # getting index corresponding to validation and test proportion\n",
    "    train_idx = int(len(df)*(1-(val_proportion + test_proportion)))\n",
    "    val_idx = int(len(df)*(1-test_proportion))\n",
    "\n",
    "    # splitting main df into train, validation and test df\n",
    "    train_df = df.iloc[:train_idx]\n",
    "    val_df = df.iloc[train_idx + 1 : val_idx]\n",
    "    test_df = df.iloc[val_idx + 1 : ]\n",
    "\n",
    "    # setting scaler to normalize features between 0 and 1 (this is used for faster convergence)\n",
    "    scaler = MinMaxScaler(feature_range=(0,1))\n",
    "    scaler = scaler.fit(train_df)\n",
    "\n",
    "    # applying scaling to dataframes\n",
    "    train_df = pd.DataFrame(scaler.transform(train_df), columns=train_df.columns)\n",
    "    val_df = pd.DataFrame(scaler.transform(val_df), columns=val_df.columns)\n",
    "    test_df = pd.DataFrame(scaler.transform(test_df), columns=test_df.columns)\n",
    "\n",
    "    # getting sequences of data for train, val and test datasets\n",
    "    X_train,y_train = create_sequences(train_df, 'label', sequence_size)\n",
    "    X_val,y_val = create_sequences(val_df, 'label', sequence_size)\n",
    "    X_test,y_test = create_sequences(test_df, 'label', sequence_size)\n",
    "\n",
    "    # setting dataset as classes (needed for dataloader)\n",
    "    train_data = Dataset(X_train,y_train)\n",
    "    val_data = Dataset(X_val,y_val)\n",
    "    test_data = Dataset(X_test,y_test)\n",
    "    \n",
    "    # creating dataloaders for train and test datasets (getting data in batches)\n",
    "    train_loader = DataLoader(train_data,batch_size=batch_size)\n",
    "    val_loader = DataLoader(val_data,batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_data,batch_size=batch_size)\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LauSpuPOvG4n"
   },
   "source": [
    "## Machine learning Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "SNwBh3f9yTdp"
   },
   "outputs": [],
   "source": [
    "# setting model as class (standard way of doing it)\n",
    "class LSTMClassifier(nn.Module):\n",
    "    # init method\n",
    "    def __init__(self, input_size, hidden_size,sequence_size,num_layers,dropout):\n",
    "        \n",
    "        # calling super constructor\n",
    "        super(LSTMClassifier,self).__init__()\n",
    "\n",
    "        # initializing all classes variables\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sequence_size = sequence_size\n",
    "        self.num_layers=num_layers\n",
    "        self.droput = dropout\n",
    "        \n",
    "        # creating the LSTM cell of specified dimensions and characteristics\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.input_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=0.2)\n",
    "\n",
    "        # dimension reduction: we want the hidden state to be reducted to single output \n",
    "        self.linear = nn.Linear(in_features=hidden_size, out_features=1)\n",
    "\n",
    "        # sigmoid function so that output is between 0 and 1 (classification)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    # forward method to set \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # applying input of LSTM cells to get output and hidden states\n",
    "        lstm_out, self.hidden = self.lstm(x)\n",
    "        \n",
    "        # getting output between 0 and 1 from last hidden state output of LSTM netword\n",
    "        y_pred = self.sigmoid(self.linear(lstm_out[:,-1,:]))\n",
    "\n",
    "        return y_pred\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "QuLsEF0gT9m3"
   },
   "outputs": [],
   "source": [
    "def accuracy(y_prob,label):\n",
    "    y_pred = [i>0.5 for i in y_prob]\n",
    "    return sum([i==j for(i,j) in zip(y_pred,label)])/len(y_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get accuracy for samples that have a sigificant trend to be taken into account\n",
    "def threashold_accuracy(y_pred_list, label_list, threashold_value):\n",
    "    \n",
    "    # initializing accuracy for all batches\n",
    "    acc = []\n",
    "    \n",
    "    # initializing count of total samples and number above threashold (in absolute value)\n",
    "    samples_count = 0\n",
    "    notable_count = 0\n",
    "    \n",
    "    # iterating over all batches of samples\n",
    "    for batch in range(len(y_pred_list)):\n",
    "        \n",
    "        # adding number of samples\n",
    "        samples_count += len(y_pred_list[batch])\n",
    "        \n",
    "        # getting indices where predictions are above threashold (in absolute value)\n",
    "        idx = abs(y_pred_list[batch]-0.5) > threashold_value\n",
    "        \n",
    "        # counting number of samples above threashold\n",
    "        notable_count += sum(idx)\n",
    "        \n",
    "        if sum(idx)!= 0:\n",
    "            \n",
    "            # getting notable predictions and the true labels \n",
    "            notable_preds = list(compress(y_pred_list[batch],idx))\n",
    "            notable_labels = list(compress(label_list[batch],idx))\n",
    "            \n",
    "            # computing batch accuracy for notable predictions and appending to global accuracy\n",
    "            batch_acc = accuracy(notable_preds,notable_labels)\n",
    "            acc.append(batch_acc)\n",
    "            \n",
    "    if len(acc) != 0:\n",
    "        # getting mean accuracy over batch\n",
    "        mean_acc = sum(acc)/len(acc)\n",
    "        \n",
    "    else:\n",
    "        # if no values above threashold\n",
    "        mean_acc = 0\n",
    "    \n",
    "    # computing number \n",
    "    prop_notable = notable_count/samples_count\n",
    "    \n",
    "    return mean_acc, prop_notable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "SEgmSP_D_ik5"
   },
   "outputs": [],
   "source": [
    "def launchML(train_dataloader, val_dataloader, input_size, hidden_size, num_layers, dropout, sequence_size,learning_rate, num_epochs, threashold_value):\n",
    "\n",
    "### INITIALIZATION ###\n",
    "\n",
    "    # initializing the model with correct dimensions\n",
    "    model = LSTMClassifier(\n",
    "        input_size=input_size,\n",
    "        hidden_size=hidden_size,\n",
    "        num_layers=num_layers,\n",
    "        sequence_size=sequence_size,\n",
    "        dropout=dropout)\n",
    "    \n",
    "    # setting criterion for loss function as Binary Cross Entropy (BCE)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # setting optimizer to ADAM with chosen learning rate \n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
    "    \n",
    "\n",
    "    # initializing loss, accuracy (simple + threasholded) and proportions of threaholded values in training and validating\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    train_acc_list = []\n",
    "    val_acc_list = []\n",
    "    thd_train_acc_list = []\n",
    "    thd_val_acc_list = []\n",
    "    train_prop_notable_list = []\n",
    "    val_prop_notable_list = []\n",
    "\n",
    "    # iterating along the number of epochs\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        \n",
    "        # initialize the predictions and labels for current epoch for training and testing\n",
    "        train_preds = []\n",
    "        train_labels = []\n",
    "        val_preds = []\n",
    "        val_labels = []\n",
    "    \n",
    "        # initialize loss and (simple) accuracy for current epoch\n",
    "        train_loss = 0\n",
    "        val_loss = 0\n",
    "        train_acc = []\n",
    "        val_acc = []\n",
    "        \n",
    "\n",
    "### TRAINING ###\n",
    "        \n",
    "        # getting model in training mode\n",
    "        model.train()\n",
    "\n",
    "        # iterating over all batches of data\n",
    "        for i,train_data in enumerate(train_dataloader):\n",
    "            \n",
    "            # getting model's predictions\n",
    "            y_pred = model(train_data[:][0].view(-1,sequence_size,input_size))\n",
    "\n",
    "            # resetting gradients to zero\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # calculating loss \n",
    "            loss = criterion(y_pred,train_data[:][1])\n",
    "\n",
    "            # backward\n",
    "            loss.backward()\n",
    "\n",
    "            # updating weights and biases of model\n",
    "            optimizer.step()\n",
    "            \n",
    "            # adding preds and labels of current batch\n",
    "            train_preds.append(y_pred)\n",
    "            train_labels.append(train_data[:][1])\n",
    "\n",
    "            # adding loss of current batch\n",
    "            train_loss+=loss.item()\n",
    "\n",
    "            # adding accuracy of current batch\n",
    "            train_acc.append(accuracy(y_pred,train_data[:][1]).item())\n",
    "            \n",
    "            \n",
    "\n",
    "### VALIDATING ###\n",
    "        \n",
    "        # getting model in evaluation mode\n",
    "        model.eval()\n",
    "\n",
    "        # iterating over all batches of data\n",
    "        for j, val_data in enumerate(val_dataloader):\n",
    "\n",
    "            # getting model's predictions\n",
    "            y_pred = model(val_data[:][0].view(-1,sequence_size,input_size))\n",
    "\n",
    "            # calculating loss \n",
    "            loss = criterion(y_pred,val_data[:][1])\n",
    "\n",
    "            # adding loss of current batch\n",
    "            val_loss+=loss.item()\n",
    "            \n",
    "            # adding preds and labels of current batch\n",
    "            val_preds.append(y_pred)\n",
    "            val_labels.append(val_data[:][1])\n",
    "\n",
    "            # adding accuracy of current batch\n",
    "            val_acc.append(accuracy(y_pred,val_data[:][1]).item())\n",
    "            \n",
    "        # getting global loss normalized over number of iterations of current epoch for training and testing\n",
    "        if i>0:\n",
    "            train_loss /= i\n",
    "        if j>0:\n",
    "            val_loss /=j\n",
    "    \n",
    "        # getting global accuracy of current epoch for training and testing\n",
    "        train_acc_epoch = sum(train_acc)/len(train_acc)\n",
    "        val_acc_epoch = sum(val_acc)/len(val_acc)\n",
    "        \n",
    "        # calculating threasholded accuracy for training and testing\n",
    "        thd_train_acc_batch, train_prop_notable_batch = threashold_accuracy(y_pred_list=train_preds, label_list=train_labels, threashold_value=threashold_value)\n",
    "        thd_val_acc_batch, val_prop_notable_batch = threashold_accuracy(y_pred_list=val_preds, label_list=val_labels, threashold_value=threashold_value)\n",
    "\n",
    "        # adding loss, accuracy (simple + threasholded) and proportions of threaholded values of current epoch to history\n",
    "        train_loss_list.append(train_loss)\n",
    "        val_loss_list.append(val_loss)\n",
    "        train_acc_list.append(train_acc_epoch)\n",
    "        val_acc_list.append(val_acc_epoch)\n",
    "        thd_train_acc_list.append(thd_train_acc_batch)\n",
    "        thd_val_acc_list.append(thd_val_acc_batch)\n",
    "        train_prop_notable_list.append(train_prop_notable_batch.item())\n",
    "        val_prop_notable_list.append(val_prop_notable_batch.item())\n",
    "        \n",
    "        # adding current model parameters to history\n",
    "        torch.save({\n",
    "            'epoch':epoch, \n",
    "            'model': model, \n",
    "            'model_state_dict': model.state_dict, \n",
    "            'train_preds':train_preds, \n",
    "            'train_labels': train_labels,\n",
    "            'val_preds': val_preds, \n",
    "            'val_labels': val_labels,\n",
    "        },os.path.join(sys.path[0],f\"pytorch_models/epoch-{epoch}.pt\"))\n",
    "        \n",
    "        # printing current status every 10%\n",
    "        if (num_epochs>10):\n",
    "            if epoch  % int(num_epochs/10) == 0:\n",
    "                print(f\"Epoch: {epoch}\\nTrain loss: {train_loss}\\tTrain accuracy:{train_acc_epoch}\\nVal loss: {val_loss}\\tVal accuracy:{val_acc_epoch}\\n-----\")\n",
    "        else:\n",
    "            print(f\"Epoch: {epoch}\\nTrain loss: {train_loss}\\tTrain accuracy:{train_acc_epoch}\\nVal loss: {val_loss}\\tVal accuracy:{val_acc_epoch}\\n-----\")\n",
    "    return [train_loss_list, val_loss_list, train_acc_list, val_acc_list, thd_train_acc_list, thd_val_acc_list, train_prop_notable_list, val_prop_notable_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hVc3T_iYJo3b"
   },
   "source": [
    "### Results formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "G9oIO40uKKa_"
   },
   "outputs": [],
   "source": [
    "# function to show graphs\n",
    "def results(train_loss, val_loss, train_acc, val_acc, train_thd_acc, val_thd_acc, train_prop, val_prop):\n",
    "    \n",
    "    # getting the actual date in string format (for saving files)\n",
    "    now = dt.datetime.strftime(dt.datetime.now(), \"%Y-%m-%d,%Hh%M\") \n",
    "    \n",
    "    # getting number of epochs\n",
    "    epochs = range(len(train_loss))\n",
    "    \n",
    "### PLOTTING LOSS ###\n",
    "    \n",
    "    # setting figure size\n",
    "    plt.figure(figsize=(16,8))\n",
    "    \n",
    "    # plotting train and val loss\n",
    "    plt.plot(epochs, train_loss, label='Train loss',lw=0.5)\n",
    "    plt.plot(epochs, val_loss, label='Val loss',lw=0.5)\n",
    "    \n",
    "    # setting labels, legend and title\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Loss function')\n",
    "    \n",
    "    # saving figure\n",
    "    plt.savefig(os.path.join(sys.path[0],f\"plots/loss[{now}].png\"))\n",
    "    \n",
    "    # showing figure\n",
    "    plt.show()\n",
    "    \n",
    "### PLOTTING SIMPLE ACCURACY ###\n",
    "    \n",
    "    # setting figure size\n",
    "    plt.figure(figsize=(16,8))\n",
    "    \n",
    "    # plotting train and val simple accuracy\n",
    "    plt.plot(epochs, train_acc, label='Train accuracy',lw=0.5)\n",
    "    plt.plot(epochs, val_acc, label='Val accuracy',lw=0.5)\n",
    "    \n",
    "    # adding 0.5 line (validation must be higher)\n",
    "    plt.axhline(y=0.5, color='r', linestyle='-',lw=0.5)\n",
    "    \n",
    "    # setting labels, legend and title\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title('Simple accuracy')\n",
    "    \n",
    "    # saving figure\n",
    "    plt.savefig(os.path.join(sys.path[0],f\"plots/accuracy_simple[{now}].png\"))\n",
    "    \n",
    "    # showing figure\n",
    "    plt.show()\n",
    "    \n",
    "### PLOTTING ACCURACY WITH THREASHOLD ###\n",
    "    \n",
    "    # setting figure size\n",
    "    plt.figure(figsize=(16,8))\n",
    "    \n",
    "    # plotting train and val accuracy with threashold\n",
    "    plt.plot(epochs, train_thd_acc, label='Train accuracy',lw=0.5)\n",
    "    plt.plot(epochs, val_thd_acc, label='Val accuracy',lw=0.5)\n",
    "    \n",
    "    # adding 0.5 line (validation must be higher)\n",
    "    plt.axhline(y=0.5, color='r', linestyle='-',lw=0.5)\n",
    "    \n",
    "    # setting labels, legend and title\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy with threashold')\n",
    "    plt.legend()\n",
    "    plt.title('Accuracy for values with threashold')\n",
    "    \n",
    "    # saving figure\n",
    "    plt.savefig(os.path.join(sys.path[0],f\"plots/accuracy_thd[{now}].png\"))\n",
    "    \n",
    "    # showing figure\n",
    "    plt.show()\n",
    "    \n",
    "### PLOTTING PROPORTION ###\n",
    "    \n",
    "    # setting figure size\n",
    "    plt.figure(figsize=(16,8))\n",
    "    \n",
    "    # plotting train and val proportion of notable samples\n",
    "    plt.plot(epochs, train_prop, label='Train accuracy',lw=0.5)\n",
    "    plt.plot(epochs, val_prop, label='Val accuracy',lw=0.5)\n",
    "    \n",
    "    \n",
    "    # setting labels, legend and title\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Proportion of notable points')\n",
    "    plt.legend()\n",
    "    plt.title('Proportion of values that are out of threashold ')\n",
    "    \n",
    "    # saving figure\n",
    "    plt.savefig(os.path.join(sys.path[0],f\"plots/prop_notable[{now}].png\"))\n",
    "    \n",
    "    # showing figure\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model chooser and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tester(test_dataloader,threashold_value,best_epoch):\n",
    "    \n",
    "    # loading optimal model\n",
    "    checkpoint = torch.load(os.path.join(sys.path[0],f\"pytorch_models/epoch-{best_epoch}.pt\"))\n",
    "    opt_model = checkpoint['model']\n",
    "    \n",
    "    # getting  loss function \n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    # getting model in evaluation mode\n",
    "    opt_model.eval()\n",
    "    \n",
    "    # initializing predictions and labels\n",
    "    y_pred_list = []\n",
    "    label_list = []\n",
    "    \n",
    "    # initializing test loss and accuracy\n",
    "    test_loss = 0\n",
    "    test_acc = []\n",
    "    \n",
    "    # iterating over all batches of data\n",
    "    for i, test_data in enumerate(test_dataloader):\n",
    "        \n",
    "        y_pred = opt_model(test_data[:][0].view(-1,opt_model.sequence_size,opt_model.input_size))\n",
    "\n",
    "        # calculating loss \n",
    "        loss = criterion(y_pred,test_data[:][1])\n",
    "\n",
    "        # adding loss of current batch\n",
    "        test_loss+=loss.item()\n",
    "\n",
    "        # adding accuracy of current batch\n",
    "        test_acc.append(accuracy(y_pred,test_data[:][1]).item())\n",
    "        \n",
    "        # adding predictions and labels\n",
    "        y_pred_list.append(y_pred)\n",
    "        label_list.append(test_data[:][1])\n",
    "    \n",
    "    # getting global loss normalized over number of iterations \n",
    "    if i>0:\n",
    "        test_loss /= i\n",
    "        \n",
    "    # getting global accuracy  \n",
    "    test_acc = sum(test_acc)/len(test_acc)\n",
    "    \n",
    "    # getting threasholded accuracy\n",
    "    thd_test_acc, test_prop_notable = threashold_accuracy(y_pred_list, label_list, threashold_value)\n",
    "    \n",
    "    print(f\"Test loss: {test_loss}\\tTest accuracy:{test_acc}\\t Test threashold accuracy:{thd_test_acc}\\t Proportion notable:{test_prop_notable.item()}\")\n",
    "    \n",
    "    # saving trained model\n",
    "    torch.save(opt_model,os.path.join(sys.path[0],\"pytorch_models/pretrained_model.pt\"))\n",
    "    \n",
    "    return test_loss, test_acc, thd_test_acc, test_prop_notable.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saver(best_epoch, parameters, hist_data,test_res):\n",
    "    # loading optimal model values\n",
    "    checkpoint = torch.load(os.path.join(sys.path[0],f\"pytorch_models/epoch-{best_epoch}.pt\"))\n",
    "                            \n",
    "    # adding all values of simulation to array\n",
    "    sim_values = []\n",
    "    sim_values.append(dt.datetime.now())\n",
    "    sim_values.extend(parameters)\n",
    "    sim_values.extend([i[best_epoch] for i in hist_data])\n",
    "    sim_values.extend(test_res)\n",
    "    out = pd.DataFrame(sim_values).transpose()\n",
    "    out.columns = ['date','TIME_INT','RSI_WINDOW','NUM_FEATURES','VAL_PROP','TEST_PROP','SEQUENCE_SIZE','BATCH_SIZE','NUM_EPOCHS','LEARNING_RATE','HIDDEN_SIZE','NUM_LAYERS','DROPOUT','THREASHOLD_VALUE','train_loss','val_loss','train_acc','val_acc','thd_train_acc','thd_val_acc','train_prop_notable','val_prop_notable','test_loss','test_acc','thd_test_acc','test_prop_notable']\n",
    "    out.to_csv(os.path.join(sys.path[0],\"data/results.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1qhEFxxQsmrD"
   },
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all changeable parameters are displayed here\n",
    "\n",
    "## data engineering parameters ##\n",
    "\n",
    "# opening global parameters file\n",
    "with open(os.path.join(sys.path[0],'parameters.json')) as fp:\n",
    "    parameters = json.load(fp)\n",
    "\n",
    "# getting global parameters\n",
    "TIME_INT = parameters['TIME_INT'] # time interval\n",
    "RSI_WINDOW = int(parameters['RSI_WINDOW']) # time window for RSI calulcations\n",
    "\n",
    "\n",
    "VAL_PROP = 0.15 # proportion of the data that will be in the validation set\n",
    "TEST_PROP = 0.15 # proportion of the data that will be in the validation set\n",
    "\n",
    "## machine learning model hyperparameters\n",
    "\n",
    "NUM_FEATURES = 6 # number of features (number of columns-1 of the dataframe)\n",
    "SEQUENCE_SIZE = 60 # number of elements to be in sequence \n",
    "BATCH_SIZE = 256 # batch size for faster computation of ML model\n",
    "\n",
    "LEARNING_RATE = 0.001 # step used for optimization\n",
    "HIDDEN_SIZE = 50 # number of\n",
    "NUM_LAYERS = 4 # number of \n",
    "DROPOUT = 0.2 #\n",
    "\n",
    "NUM_EPOCHS = 10 #number of epochs\n",
    "\n",
    "## parameter for accuracy (can be changed after ML)\n",
    "THREASHOLD_VALUE = 0.1 # value of the threashold for the variation to be described as going up or down (in percent)\n",
    "\n",
    "# putting all parameters in a list\n",
    "PARAMETERS = [TIME_INT,RSI_WINDOW,NUM_FEATURES,VAL_PROP,TEST_PROP,SEQUENCE_SIZE,BATCH_SIZE,NUM_EPOCHS,LEARNING_RATE,HIDDEN_SIZE,NUM_LAYERS,DROPOUT,THREASHOLD_VALUE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p4pmhE6jvcXg"
   },
   "source": [
    "## Launching all functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing and showing dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "_w03KcXEvexY"
   },
   "outputs": [],
   "source": [
    "df = initialize(raw_sentiment=raw_sentiment_data, raw_prices=raw_bitcoin_data, time_int=TIME_INT,rsi_window=RSI_WINDOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>neg</th>\n",
       "      <th>pos</th>\n",
       "      <th>tweets</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>rsi</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.095429</td>\n",
       "      <td>0.140714</td>\n",
       "      <td>7</td>\n",
       "      <td>8689.25</td>\n",
       "      <td>277.624216</td>\n",
       "      <td>64.506114</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.046000</td>\n",
       "      <td>0.045600</td>\n",
       "      <td>5</td>\n",
       "      <td>8705.99</td>\n",
       "      <td>206.281545</td>\n",
       "      <td>67.970423</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.057200</td>\n",
       "      <td>0.162000</td>\n",
       "      <td>5</td>\n",
       "      <td>8697.86</td>\n",
       "      <td>126.461554</td>\n",
       "      <td>64.747779</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.101000</td>\n",
       "      <td>5</td>\n",
       "      <td>8677.28</td>\n",
       "      <td>104.150270</td>\n",
       "      <td>57.501490</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.290500</td>\n",
       "      <td>2</td>\n",
       "      <td>8677.19</td>\n",
       "      <td>60.670470</td>\n",
       "      <td>57.471956</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702715</th>\n",
       "      <td>0.006091</td>\n",
       "      <td>0.059273</td>\n",
       "      <td>11</td>\n",
       "      <td>49512.47</td>\n",
       "      <td>54.527919</td>\n",
       "      <td>52.739861</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702716</th>\n",
       "      <td>0.039556</td>\n",
       "      <td>0.119000</td>\n",
       "      <td>9</td>\n",
       "      <td>49636.82</td>\n",
       "      <td>47.251165</td>\n",
       "      <td>57.019137</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702717</th>\n",
       "      <td>0.050143</td>\n",
       "      <td>0.124643</td>\n",
       "      <td>14</td>\n",
       "      <td>49635.26</td>\n",
       "      <td>47.492005</td>\n",
       "      <td>56.951209</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702718</th>\n",
       "      <td>0.048333</td>\n",
       "      <td>0.031222</td>\n",
       "      <td>9</td>\n",
       "      <td>49681.26</td>\n",
       "      <td>62.746752</td>\n",
       "      <td>58.482574</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>702719</th>\n",
       "      <td>0.074077</td>\n",
       "      <td>0.111538</td>\n",
       "      <td>13</td>\n",
       "      <td>49676.20</td>\n",
       "      <td>40.878492</td>\n",
       "      <td>58.243273</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>702720 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             neg       pos  tweets     close      volume        rsi  label\n",
       "0       0.095429  0.140714       7   8689.25  277.624216  64.506114      0\n",
       "1       0.046000  0.045600       5   8705.99  206.281545  67.970423      0\n",
       "2       0.057200  0.162000       5   8697.86  126.461554  64.747779      1\n",
       "3       0.000000  0.101000       5   8677.28  104.150270  57.501490      1\n",
       "4       0.000000  0.290500       2   8677.19   60.670470  57.471956      1\n",
       "...          ...       ...     ...       ...         ...        ...    ...\n",
       "702715  0.006091  0.059273      11  49512.47   54.527919  52.739861      1\n",
       "702716  0.039556  0.119000       9  49636.82   47.251165  57.019137      0\n",
       "702717  0.050143  0.124643      14  49635.26   47.492005  56.951209      1\n",
       "702718  0.048333  0.031222       9  49681.26   62.746752  58.482574      0\n",
       "702719  0.074077  0.111538      13  49676.20   40.878492  58.243273      1\n",
       "\n",
       "[702720 rows x 7 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting data in dataloader (ready for ML model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wXnRj6DgypfK"
   },
   "outputs": [],
   "source": [
    "train_dataloader, val_dataloader, test_dataloader = df2loader(df,val_proportion=VAL_PROP,test_proportion=TEST_PROP, sequence_size=SEQUENCE_SIZE, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launching model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 746,
     "referenced_widgets": [
      "2657cedb52bc43febb7dc976f18195ac",
      "454218fc9beb4a7d8b4c093493ccd4c5",
      "31030b32567844f7bd52892bed07871a",
      "831c996f399f4b789b295d3a4e26c94e",
      "2110d390bb99484ca6c86d26a0d4a49c",
      "a1f0a04b18364ee1987351e56b53577a",
      "829ebfef524a4a93b70967242f24468f",
      "1a8d712ab84c491c963a52bee15406f2"
     ]
    },
    "id": "6YWigC8W5BRf",
    "outputId": "5bf962d4-410a-4424-ff43-abe974af52a6"
   },
   "outputs": [],
   "source": [
    "hist_data = launchML(train_dataloader=train_dataloader,val_dataloader=val_dataloader, input_size=NUM_FEATURES, hidden_size=HIDDEN_SIZE,num_layers=NUM_LAYERS,dropout=DROPOUT, sequence_size=SEQUENCE_SIZE,learning_rate=LEARNING_RATE, num_epochs=NUM_EPOCHS,threashold_value=THREASHOLD_VALUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 977
    },
    "id": "oHUjg6EQ5seQ",
    "outputId": "0f3a9fcf-3e33-4f2f-84a4-3a4602d14b41"
   },
   "outputs": [],
   "source": [
    "results(hist_data[0],hist_data[1],hist_data[2],hist_data[3],hist_data[4],hist_data[5],hist_data[6],hist_data[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SdRyIoE9PaGI"
   },
   "outputs": [],
   "source": [
    "test_res = tester(test_dataloader=test_dataloader,threashold_value=THREASHOLD_VALUE, best_epoch=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver(best_epoch=50, parameters=PARAMETERS, hist_data=hist_data,test_res=test_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "modeller.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1a8d712ab84c491c963a52bee15406f2": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2110d390bb99484ca6c86d26a0d4a49c": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "2657cedb52bc43febb7dc976f18195ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_31030b32567844f7bd52892bed07871a",
       "IPY_MODEL_831c996f399f4b789b295d3a4e26c94e"
      ],
      "layout": "IPY_MODEL_454218fc9beb4a7d8b4c093493ccd4c5"
     }
    },
    "31030b32567844f7bd52892bed07871a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a1f0a04b18364ee1987351e56b53577a",
      "max": 50,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2110d390bb99484ca6c86d26a0d4a49c",
      "value": 50
     }
    },
    "454218fc9beb4a7d8b4c093493ccd4c5": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "829ebfef524a4a93b70967242f24468f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "831c996f399f4b789b295d3a4e26c94e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1a8d712ab84c491c963a52bee15406f2",
      "placeholder": "​",
      "style": "IPY_MODEL_829ebfef524a4a93b70967242f24468f",
      "value": " 50/50 [58:25&lt;00:00, 70.12s/it]"
     }
    },
    "a1f0a04b18364ee1987351e56b53577a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
